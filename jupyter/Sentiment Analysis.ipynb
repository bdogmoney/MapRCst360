{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with TextBlob\n",
    "\n",
    "Sentiment analysis is a method for discerning emotions in text. [TextBlob](https://textblob.readthedocs.io/en/latest/index.html) is a python library for processing textual data. I'm using it here to perfom sentiment analysis based on the Naive Bayes analyzer which is a model pre-trained on a dataset of movie reviews.  \n",
    "\n",
    "In this notebook I explore two different textual datasets. First I experiment with the Twitter API to analyze a small dataset. Second I use analyze a massive textual dataset from Ubuntu IRC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets percentage: 14 %\n",
      "Negative tweets percentage: 0 %\n",
      "Neutral tweets percentage: 85 %\n",
      "\n",
      "\n",
      "Positive tweets:\n",
      "RT @iandownard: @KirkDBorne @mapr Here's a bit of Drill in Action &gt; https://t.co/AYY0vBQz0U\n",
      "\n",
      "\n",
      "Negative tweets:\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    " \n",
    "class TwitterClient(object):\n",
    "    '''\n",
    "    Generic Twitter Class for sentiment analysis.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class constructor or initialization method.\n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console\n",
    "        consumer_key = 'djCoU87E8ziezWSZjaHmGOFNv'\n",
    "        consumer_secret = 'WJe8Qotu4agT777IjusYD6OZmnxJeY2tEUrpj1Me4Q49JQqSaB'\n",
    "        access_token = '2813666233-1zmbrE7Eij1m9AIzJcYEuWya3LaHPHXVTqLTQE7'\n",
    "        access_token_secret = 'mVUPzUQDlccwGmWFCsq9doRRXRJAfdvjvIcaqElOkYrB5'\n",
    " \n",
    "        # attempt authentication\n",
    "        try:\n",
    "            # create OAuthHandler object\n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            # set access token and secret\n",
    "            self.auth.set_access_token(access_token, access_token_secret)\n",
    "            # create tweepy API object to fetch tweets\n",
    "            self.api = tweepy.API(self.auth)\n",
    "        except:\n",
    "            print(\"Error: Authentication Failed\")\n",
    " \n",
    "    def clean_tweet(self, tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) | (\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def get_tweet_sentiment(self, tweet):\n",
    "        '''\n",
    "        Utility function to classify sentiment of passed tweet\n",
    "        using textblob's sentiment method\n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        # set sentiment\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    " \n",
    "    def get_tweets(self, query, count = 10):\n",
    "        '''\n",
    "        Main function to fetch tweets and parse them.\n",
    "        '''\n",
    "        # empty list to store parsed tweets\n",
    "        tweets = []\n",
    " \n",
    "        try:\n",
    "            # call twitter api to fetch tweets\n",
    "            fetched_tweets = self.api.search(q = query, count = count)\n",
    " \n",
    "            # parsing tweets one by one\n",
    "            for tweet in fetched_tweets:\n",
    "                # empty dictionary to store required params of a tweet\n",
    "                parsed_tweet = {}\n",
    " \n",
    "                # saving text of tweet\n",
    "                parsed_tweet['text'] = tweet.text\n",
    "                # saving sentiment of tweet\n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
    " \n",
    "                # appending parsed tweet to tweets list\n",
    "                if tweet.retweet_count > 0:\n",
    "                    # if tweet has retweets, ensure that it is appended only once\n",
    "                    if parsed_tweet not in tweets:\n",
    "                        tweets.append(parsed_tweet)\n",
    "                else:\n",
    "                    tweets.append(parsed_tweet)\n",
    " \n",
    "            # return parsed tweets\n",
    "            return tweets\n",
    " \n",
    "        except tweepy.TweepError as e:\n",
    "            # print error (if any)\n",
    "            print(\"Error : \" + str(e))\n",
    " \n",
    "def main():\n",
    "    # creating object of TwitterClient Class\n",
    "    api = TwitterClient()\n",
    "    # calling function to get tweets\n",
    "    tweets = api.get_tweets(query = '@iandownard', count = 200)\n",
    " \n",
    "    # picking positive tweets from tweets\n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive']\n",
    "    # percentage of positive tweets\n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets)))\n",
    "    # picking negative tweets from tweets\n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']\n",
    "    # percentage of negative tweets\n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets)))\n",
    "    # percentage of neutral tweets\n",
    "    print(\"Neutral tweets percentage: {} %\".format(100*(len(tweets) - len(ntweets) - len(ptweets))/len(tweets)))\n",
    "\n",
    "    # printing first 5 positive tweets\n",
    "    print(\"\\n\\nPositive tweets:\")\n",
    "    for tweet in ptweets[:10]:\n",
    "        print(tweet['text'])\n",
    " \n",
    "    # printing first 5 negative tweets\n",
    "    print(\"\\n\\nNegative tweets:\")\n",
    "    for tweet in ntweets[:10]:\n",
    "        print(tweet['text'])\n",
    " \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # calling main function\n",
    "   main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis from Ubuntu IRC data\n",
    "\n",
    "The dataset I'm using is the [Ubuntu Dialogue Corpus v2.0] which consists of two-person chat conversations from an Ubuntu IRC channel. \n",
    "\n",
    "Here's how I generated the dataset:\n",
    "\n",
    "```\n",
    "git clone https://github.com/rkadlec/ubuntu-ranking-dataset-creator\n",
    "pip install unicodecsv\n",
    "cd src\n",
    "./generate.sh -t -s -l\n",
    "tar -xzvf ubuntu_dialogs.tgz\n",
    "```\n",
    "\n",
    "## References: \n",
    "[http://irclogs.ubuntu.com](http://irclogs.ubuntu.com)<br>\n",
    "[https://arxiv.org/pdf/1506.08909.pdf](https://arxiv.org/pdf/1506.08909.pdf)<br>\n",
    "[https://github.com/rkadlec/ubuntu-ranking-dataset-creator](https://github.com/rkadlec/ubuntu-ranking-dataset-creator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='pos', p_pos=0.5449551130839605, p_neg=0.45504488691603534)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob import formats\n",
    "#from textblob.classifiers import NaiveBayesAnalyzer\n",
    "import csv\n",
    "\n",
    "text=\"\"\n",
    "with open('/Users/idownard/tmp/ubuntu-ranking-dataset-creator/src/dialogs/9/1.tsv', 'r') as f:\n",
    "    reader=csv.reader(f,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        text = text+\". \"+(row[-1])\n",
    "\n",
    "blob = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
    "print(blob.sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
